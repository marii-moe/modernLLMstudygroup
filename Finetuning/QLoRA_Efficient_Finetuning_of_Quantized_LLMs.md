 # QLoRA: Efficient Finetuning of Quantized LLMs

This Friday week we will be reading "[QLoRA: Efficient Finetuning of Quantized LLMs](https://arxiv.org/abs/2305.14314)". QLoRA introduced a way to save memory using quantization in LoRA training. They also introduce the Guanaco family of models, and do some analyses on fine-tuning data.

Further Reading:
 * LoRA: Low-Rank Adaptation of Large Language Models - finetuning with adapters if you missed it ([arxiv.org/abs/2106.09685](https://arxiv.org/abs/2106.09685))

 * Make Pre-trained Model Reversible [arxiv.org/abs/2306.00477](https://arxiv.org/abs/2306.00477) - another LoRA memory effiencient technique removing need to store activations. 