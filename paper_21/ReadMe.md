# DoReMi: Optimizing Data Mixtures Speeds Up Language Model Pretraining

this week's paper is and discuss [*DoReMi: Optimizing Data Mixtures Speeds Up Language Model Pretraining*](<https://arxiv.org/abs/2305.10429>). Which first trains a small proxy model using group distributionally robust optimization (Group DRO) over domains to produce domain weights (mixture proportions) without knowledge of downstream tasks. Then resample a dataset with these domain weights and train a larger, full-sized model.

Further Reading:
* [*Data Selection for Language Models via Importance Resampling*](<https://arxiv.org/abs/2302.03169>)
* [*When Less is More: Investigating Data Pruning for Pretraining LLMs at Scale*](<https://arxiv.org/abs/2309.04564>)
* [*Don't Stop Pretraining: Adapt Language Models to Domains and Tasks*](<https://arxiv.org/abs/2004.10964>)
* [*DsDm: Model-Aware Dataset Selection with Datamodels*](<https://arxiv.org/abs/2401.12926>)