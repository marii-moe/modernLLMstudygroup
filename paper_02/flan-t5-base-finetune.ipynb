{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finetune T5-base"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This example demonstrates \"how to fine-tune [google/flan-t5-base](https://huggingface.co/google/flan-t5-base) for chat & dialogue. Basically follows this [walk-thru by Philipp Schmid](https://www.philschmid.de/fine-tune-flan-t5)\n",
    "\n",
    "Papers: \n",
    "\n",
    "[Finetuned Language Models are Zero-Shot Learners](https://arxiv.org/abs/2109.01652) \\\n",
    "[Scaling Instruction-Finetuned Language Models](https://arxiv.org/pdf/2210.11416.pdf) \\\n",
    "[The Flan Collection: Designing Data and Methods for Effective Instruction Tuning](https://arxiv.org/abs/2301.13688)\n",
    "\n",
    "Code: \n",
    "\n",
    "https://github.com/google-research/FLAN\n",
    "\n",
    "Metrics:\n",
    "\n",
    "[ROUGE (metric)](https://en.wikipedia.org/wiki/ROUGE_(metric)) \\\n",
    "[What is the ROUGE metric (video from HF course)](https://www.youtube.com/watch?v=TMshhnrEXlg) \\\n",
    "[An intro to ROUGE, and how to use it to evaluate summaries](https://www.freecodecamp.org/news/what-is-rouge-and-how-it-works-for-evaluation-of-summaries-e059fb8ac840/) \\\n",
    "[Two minutes NLP — Learn the ROUGE metric by examples](https://medium.com/nlplanet/two-minutes-nlp-learn-the-rouge-metric-by-examples-f179cc285499)\n",
    "\n",
    "Other Resources:\n",
    "\n",
    "[HF NLP Course - Part 7: Summarization](https://huggingface.co/learn/nlp-course/en/chapter7/5?fw=pt#summarization) \\\n",
    "[Training summarization & translation models with fastai & blurr: W&B Study Group](https://www.youtube.com/watch?v=Jsz4E2iNXUA)\n",
    "\n",
    "Notes:\n",
    "\n",
    "* \"These models have been fine-tuned on more that 1000 additional tasks covering also more languages\" \\\n",
    "* Improves upon T5 with instruction finetuning with more tasks and including chain-of-thought data \\\n",
    "* Dataset = [samsum](https://huggingface.co/datasets/samsum) (\"16k messenger-like conversations with summaries\") \\"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# NOTE: To limit HF's Trainer/Accelerate from using all the GPU's, you need to set this environment var BEFORE you import any\n",
    "# related package!!!\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import randrange\n",
    "\n",
    "from datasets import concatenate_datasets, load_dataset\n",
    "import evaluate\n",
    "from huggingface_hub import HfFolder\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "import numpy as np\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, DataCollatorForSeq2Seq, Seq2SeqTrainer, Seq2SeqTrainingArguments\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download(\"punkt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ignore_tok_id = -100\n",
    "\n",
    "dataset_id = \"samsum\"\n",
    "model_id = \"google/flan-t5-base\"\n",
    "hf_repo_id = f\"{model_id.split('/')[1]}-{dataset_id}\"\n",
    "\n",
    "hf_tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "hf_model = AutoModelForSeq2SeqLM.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_training_ds = load_dataset(dataset_id)\n",
    "\n",
    "raw_training_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Train dataset size: {len(raw_training_ds['train'])}\")\n",
    "print(f\"Test dataset size: {len(raw_training_ds['test'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = raw_training_ds['train'][randrange(len(raw_training_ds[\"train\"]))]\n",
    "\n",
    "print(f\"dialogue: \\n{sample['dialogue']}\\n---------------\")\n",
    "print(f\"summary: \\n{sample['summary']}\\n---------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import concatenate_datasets\n",
    "\n",
    "# The maximum total input sequence length after tokenization.\n",
    "# Sequences longer than this will be truncated, sequences shorter will be padded.\n",
    "tokenized_inputs = concatenate_datasets([raw_training_ds[\"train\"], raw_training_ds[\"test\"]]).map(\n",
    "    lambda x: hf_tokenizer(x[\"dialogue\"], truncation=True), batched=True, remove_columns=[\"dialogue\", \"summary\"]\n",
    ")\n",
    "\n",
    "source_lengths = [len(x) for x in tokenized_inputs[\"input_ids\"]]\n",
    "max_source_length = max(source_lengths)\n",
    "print(f\"Max source length: {max_source_length}\")\n",
    "\n",
    "# The maximum total sequence length for target text after tokenization.\n",
    "# Sequences longer than this will be truncated, sequences shorter will be padded.\"\n",
    "tokenized_targets = concatenate_datasets([raw_training_ds[\"train\"], raw_training_ds[\"test\"]]).map(\n",
    "    lambda x: hf_tokenizer(x[\"summary\"], truncation=True), batched=True, remove_columns=[\"dialogue\", \"summary\"]\n",
    ")\n",
    "\n",
    "target_lengths = [len(x) for x in tokenized_targets[\"input_ids\"]]\n",
    "max_target_length = max(target_lengths)\n",
    "print(f\"Max target length: {max_target_length}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 2, figsize=(10,3.5), sharey=True)\n",
    "\n",
    "ax[0].hist(source_lengths, bins=20, color=\"C0\", edgecolor=\"C0\")\n",
    "ax[0].set_title(\"Dialogue Token Length\")\n",
    "ax[0].set_xlabel(\"Length\")\n",
    "ax[0].set_ylabel(\"Count\")\n",
    "\n",
    "ax[1].hist(target_lengths, bins=20, color=\"C0\", edgecolor=\"C0\")\n",
    "ax[1].set_title(\"Summary Length\")\n",
    "ax[1].set_xlabel(\"Length\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"In T5, every NLP task is formulated in terms of a prompt prefix like summarize: which conditions the model to adapt the generated text to the prompt.\"\n",
    "\n",
    "![Image Alt Text](https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter7/t5.svg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_examples(sample, padding=False):\n",
    "    # add prefix to the input for t5\n",
    "    inputs = [\"summarize: \" + item for item in sample[\"dialogue\"]]\n",
    "\n",
    "    # tokenize inputs\n",
    "    model_inputs = hf_tokenizer(inputs, max_length=max_source_length, padding=padding, truncation=True)\n",
    "\n",
    "    # Tokenize targets with the `text_target` keyword argument\n",
    "    labels = hf_tokenizer(text_target=sample[\"summary\"], max_length=max_target_length, padding=padding, truncation=True)\n",
    "\n",
    "    # If we are padding here, replace all tokenizer.pad_token_id in the labels by -100 when we want to ignore padding in the loss.\n",
    "    if padding == \"max_length\":\n",
    "        labels[\"input_ids\"] = [[(l if l != hf_tokenizer.pad_token_id else ignore_tok_id) for l in label] for label in labels[\"input_ids\"]]\n",
    "\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    \n",
    "    return model_inputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tok_training_ds = raw_training_ds.map(preprocess_examples, batched=True, remove_columns=[\"dialogue\", \"summary\", \"id\"])\n",
    "\n",
    "print(f\"Keys of tokenized dataset: {list(tok_training_ds['train'].features)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.init(project=f\"llms_ft_t5_base_samsum\")  # Replace 'project_name' with your project name in wandb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Metrics\n",
    "\n",
    "`ROUGE` is \"developed for applications like summarization where high recall is more important than just precision ... we check how many n-grams in the reference text also occur in the generated text\"\n",
    "\n",
    "Recall= # of overlapping words / # words in reference summary\n",
    "Precision = # of overlapping words / # of words in generated summary\n",
    "​\n",
    "\n",
    "\n",
    "\n",
    "The score that is reported is generally the F1 for each rouge sub-metric (e.g., the harmonic mean of the precision and recall scores)\n",
    "\n",
    "**Longest Common Substring (LCS) Score:** \n",
    "\n",
    "`ROUGE-L` = Calculates the score per sentence and averages it for the summaries \\\n",
    "`ROUGE-LSUM` = Calculates it directly over the whole summary\n",
    "\n",
    "Note that LCS is normalized to account for reference summaries of different legnths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rouge_score = evaluate.load(\"rouge\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to postprocess text\n",
    "def postprocess_text(preds, labels):\n",
    "    preds = [pred.strip() for pred in preds]\n",
    "    labels = [label.strip() for label in labels]\n",
    "\n",
    "    # rougeLSum expects newline after each sentence\n",
    "    preds = [\"\\n\".join(sent_tokenize(pred)) for pred in preds]\n",
    "    labels = [\"\\n\".join(sent_tokenize(label)) for label in labels]\n",
    "\n",
    "    return preds, labels\n",
    "\n",
    "\n",
    "def compute_metrics(eval_preds):\n",
    "    preds, labels = eval_preds\n",
    "    if isinstance(preds, tuple):\n",
    "        preds = preds[0]\n",
    "        \n",
    "    decoded_preds = hf_tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "    \n",
    "    # Replace -100 in the labels as we can't decode them.\n",
    "    labels = np.where(labels != ignore_tok_id, labels, hf_tokenizer.pad_token_id)\n",
    "    decoded_labels = hf_tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "    # Ensure generated text is formatted correctly for rouge\n",
    "    decoded_preds, decoded_labels = postprocess_text(decoded_preds, decoded_labels)\n",
    "\n",
    "    result = rouge_score.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=True)\n",
    "    result = {k: round(v * 100, 4) for k, v in result.items()}\n",
    "    \n",
    "    prediction_lens = [np.count_nonzero(pred != hf_tokenizer.pad_token_id) for pred in preds]\n",
    "    result[\"gen_len\"] = np.mean(prediction_lens)\n",
    "    \n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: DataCollator\n",
    "\n",
    "During training we use \"teacher forcing\" on the decoder side so that the current and previous tokens are predicting the next token in the summary.  We do this by shifting the labels to the right by 1. This in conjunction with the masked self-attention mechanism ensures that we aren't seeing future tokens when we make a prediction at each time step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: If you use mixed precision all your tensors need to have dimensions that are multiple of 8 (thus we set `pad_to_multiple_of` = 8 just in case)\n",
    "data_collator = DataCollatorForSeq2Seq(hf_tokenizer, model=hf_model, label_pad_token_id=ignore_tok_id, pad_to_multiple_of=8)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define training args\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=f\"./{hf_repo_id}\",\n",
    "    overwrite_output_dir=True,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=3,\n",
    "    learning_rate=3e-5,\n",
    "    # Overflows with fp16\n",
    "    fp16=False,  \n",
    "    # So we can evaluate generations as part of the training loop (uses `generate()` instead of model's forward pass to create preds)\n",
    "    predict_with_generate=True,  \n",
    "    # --- logging & evaluation strategies ---\n",
    "    logging_dir=f\"./{hf_repo_id}/logs\",\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=500,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    save_total_limit=2,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"rougeLsum\",\n",
    "    # --- push to hub parameters ---\n",
    "    report_to=\"wandb\",\n",
    "    push_to_hub=True,\n",
    "    hub_strategy=\"every_save\",\n",
    "    hub_model_id=hf_repo_id,\n",
    "    hub_token=HfFolder.get_token(),\n",
    ")\n",
    "\n",
    "# Create Trainer instance\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=hf_model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=tok_training_ds[\"train\"],\n",
    "    eval_dataset=tok_training_ds[\"test\"],\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How are we doing BEFORE training\n",
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save our tokenizer and create model card\n",
    "hf_tokenizer.save_pretrained(hf_repo_id)\n",
    "trainer.create_model_card()\n",
    "\n",
    "# Push the results to the hub\n",
    "trainer.push_to_hub()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "from random import randrange\n",
    "\n",
    "# load model and tokenizer from huggingface hub with pipeline\n",
    "summarizer = pipeline(\"summarization\", model=\"wgpubs/flan-t5-base-samsum\", device=0)\n",
    "\n",
    "# select a random test sample\n",
    "sample = raw_training_ds['test'][randrange(len(raw_training_ds[\"test\"]))]\n",
    "print(f\"dialogue: \\n{sample['dialogue']}\\n---------------\")\n",
    "\n",
    "# summarize dialogue\n",
    "res = summarizer(sample[\"dialogue\"])\n",
    "\n",
    "print(f\"flan-t5-base summary:\\n{res[0]['summary_text']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
