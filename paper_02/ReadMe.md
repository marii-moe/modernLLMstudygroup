# Finetuned Language Models Are Zero-Shot Learners

This week's paper is [*Finetuned Language Models Are Zero-Shot Learners*](https://arxiv.org/abs/2109.01652) on supervised fine-tuning.

Further Reading:
- [*Multitask Prompted Training Enables Zero-Shot Task Generalization*](https://arxiv.org/abs/2110.08207)