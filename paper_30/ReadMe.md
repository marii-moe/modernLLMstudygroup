# ORPO: Monolithic Preference Optimization without Reference Model

This week's paper is [*ORPO: Monolithic Preference Optimization without Reference Model*](https://arxiv.org/abs/2403.07691)

Further Reading:
- [*Provably Robust DPO: Aligning Language Models with Noisy Feedback*](https://arxiv.org/abs/2403.00409)
- [*Insights into Alignment: Evaluating DPO and its Variants Across Multiple Tasks*](https://arxiv.org/abs/2404.14723)
- [ORPO math derivation](https://colab.research.google.com/drive/1dgR8O4pbuvecVEkfq6xf_dHwVn2G9Kjf?usp=sharing)
- [Usefull ORPO training details](https://x.com/maximelabonne/status/1781068954087104686)