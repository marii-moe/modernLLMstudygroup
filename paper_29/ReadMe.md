# KTO: Model Alignment as Prospect Theoretic Optimization

This week's paper is [*KTO: Model Alignment as Prospect Theoretic Optimization*](https://arxiv.org/abs/2402.01306). This explores removing the constraint of needing preference pairs in PPO. 

Further Reading:
- [*Aligning Diffusion Models by Optimizing Human Utility*](https://arxiv.org/abs/2404.04465) (uses KTO with diffusion models)
- [KTO math derivations](https://colab.research.google.com/drive/15oV-bElA0RbwnWZNOORny0TuHD1xl7EA?usp=sharing)
- [*Pretraining Language Models with Human Preferences*](https://arxiv.org/abs/2302.08582) (survey of RLHF before DPO)
- [*Self-Rewarding Language Models*](https://arxiv.org/abs/2401.10020) (different technique self play training)
- [Preference Tuning LLMs with Direct Preference Optimization Methods](https://huggingface.co/blog/pref-tuning)
- [Orca-Math](https://x.com/ethayarajh/status/1763730480871215249) compares DPO & KTO

