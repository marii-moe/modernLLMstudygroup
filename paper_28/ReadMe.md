# Direct Preference Optimization: Your Language Model is Secretly a Reward Model

This week's paper, [*Direct Preference Optimization: Your Language Model is Secretly a Reward Model*](https://arxiv.org/abs/2305.18290), removes PPO from the equation and replaces it with a new method called DPO.

Futher Reading:
- [CDPO](https://ericmitchell.ai/cdpo.pdf)
- [Colab Notebook](https://colab.research.google.com/drive/1dLFQArmmXGVRVclyenAFxnzQG64M_V7s?usp=sharing)
- [*Direct Nash Optimization: Teaching Language Models to Self-Improve with General Preferences*](https://arxiv.org/abs/2404.03715)
- [*A General Theoretical Paradigm to Understand Learning from Human Preferences*](https://arxiv.org/abs/2310.12036)

