# D4: Improving LLM Pretraining via Document De-Duplication and Diversification

This week's paper is [*D4: Improving LLM Pretraining via Document De-Duplication and Diversification*](https://arxiv.org/abs/2308.12284). D4 explores beyond the standard MinHash deduplication, applying [*SemDeDup: Data-efficient learning at web-scale through semantic deduplication*](https://arxiv.org/abs/2303.09540) and SSL Prototype from [Beyond neural scaling laws: beating power law scaling via data pruning](https://arxiv.org/abs/2206.14486) to text documents to achieve better pre-training perplexity and downstream accuracy on a fixed training budget.

Further Reading:
* [*Data Selection for Language Models via Importance Resampling*](https://arxiv.org/abs/2302.03169)
* [*DoReMi: Optimizing Data Mixtures Speeds Up Language Model Pretraining*](https://arxiv.org/abs/2305.10429)
* [*When Less is More: Investigating Data Pruning for Pretraining LLMs at Scale*](https://arxiv.org/abs/2309.04564)
* [*Don't Stop Pretraining: Adapt Language Models to Domains and Tasks*](https://arxiv.org/abs/2004.10964)
* [*DsDm: Model-Aware Dataset Selection with Datamodels*](https://arxiv.org/abs/2401.12926)