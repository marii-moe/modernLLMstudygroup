# Cosmopedia

This week we will discuss [Cosmopedia](https://huggingface.co/collections/HuggingFaceTB/cosmopedia-65d4e44c693d9451ce4344d6), the 35 billion token synthetic textbooks, blogposts, stories, posts, and WikiHow articles dataset generated by Mixtral-8x7B-Instruct-v0.1.

* [Cosmpdia Blog Post](https://huggingface.co/blog/cosmopedia)
* Cosmopedia Datasets: [Full](https://huggingface.co/datasets/HuggingFaceTB/cosmopedia) and [100K sample](https://huggingface.co/datasets/HuggingFaceTB/cosmopedia-100k)
* Cosmopedia [GitHub repository](https://github.com/huggingface/cosmopedia)
* [cosmo-1B](https://huggingface.co/HuggingFaceTB/cosmo-1b): a Cosmopedia pretrained model