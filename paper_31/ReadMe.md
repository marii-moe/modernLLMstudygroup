# RewardBench: Evaluating Reward Models for Language Modeling

This week's paper is [*RewardBench: Evaluating Reward Models for Language Modeling*](https://arxiv.org/abs/2403.13787).

RewardBench is the first toolkit for benchmarking reward models. In addition to the benchmark, the authors compare scaling, test reasoning capabilities, highlight three buckets of refusal behavior, and share details on the inner workings of RMs.

Further Reading:
- [Updated benchmark](https://huggingface.co/spaces/allenai/reward-bench)
- [Video on RewardBench by the author](https://www.youtube.com/watch?v=CAaHAfCqrBA)
- [It's not PPO > DPO, It's policy generated data > stale data](https://twitter.com/natolambert/status/1783522867877134416) twitter thread
