# Training language models to follow instructions with human feedback

This week's paper [*Training language models to follow instructions with human feedback*](https://arxiv.org/abs/2203.02155).

*Learning to summarize from human feedback* is an earlier OpenAI RLHF paper which has a good video summary.

Further Reading:
- [Learning to summarize with human feedback (blog post)](https://openai.com/research/learning-to-summarize-with-human-feedback)
- [*Learning to summarize from human feedback*](https://arxiv.org/abs/2009.01325)
- [Learning to summarize from human feedback (Paper Explained)](https://www.youtube.com/watch?v=vLTmnaMpQCs)

- [Illustrating Reinforcement Learning from Human Feedback (RLHF)](https://huggingface.co/blog/rlhf)
- [RLHF: Reinforcement Learning from Human Feedback](https://huyenchip.com/2023/05/02/rlhf.html)
- [Understanding Reinforcement Learning from Human Feedback (RLHF)](https://wandb.ai/ayush-thakur/RLHF/reports/Understanding-Reinforcement-Learning-from-Human-Feedback-RLHF-Part-1--VmlldzoyODk5MTIx)
- [RLHF learning resources in 2024](https://www.interconnects.ai/p/rlhf-resources)
- [Why reward models are key for alignment](https://www.interconnects.ai/p/why-reward-models-matter)

- [Hugging Face RL Course: PPO](https://huggingface.co/learn/deep-rl-course/unit8/introduction)