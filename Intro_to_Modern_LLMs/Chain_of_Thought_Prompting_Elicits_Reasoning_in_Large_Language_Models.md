# Chain-of-Thought Prompting Elicits Reasoning in Large Language Models

This week's paper is [*Chain-of-Thought Prompting Elicits Reasoning in Large Language Models*](https://arxiv.org/abs/2201.11903).

Further Reading:
- [*Self-Consistency Improves Chain of Thought Reasoning in Language Models*](https://arxiv.org/abs/2203.11171)
- [*Large Language Models are Zero-Shot Reasoners*](https://arxiv.org/abs/2205.11916)
- [*Tree of Thoughts: Deliberate Problem Solving with Large Language Models*](https://arxiv.org/abs/2305.10601)
- [*Towards Understanding Chain-of-Thought Prompting: An Empirical Study of What Matters*](https://arxiv.org/abs/2212.10001)
- [*Measuring Faithfulness in Chain-of-Thought Reasoning*](https://arxiv.org/abs/2307.13702)
- [*Question Decomposition Improves the Faithfulness of Model-Generated Reasoning*](https://arxiv.org/abs/2307.11768)
- [*Invalid Logic, Equivalent Gains: The Bizarreness of Reasoning in Language Model Prompting*](https://arxiv.org/abs/2307.10573)
- [*Language Models Don't Always Say What They Think: Unfaithful Explanations in Chain-of-Thought Prompting*](https://arxiv.org/abs/2305.04388)