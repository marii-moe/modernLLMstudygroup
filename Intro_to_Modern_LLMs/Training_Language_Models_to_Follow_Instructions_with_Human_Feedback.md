# Training language models to follow instructions with human feedback

This week's paper is [*Training language models to follow instructions with human feedback*](https://arxiv.org/abs/2203.02155). OpenAI also wrote a [blog post](https://openai.com/blog/instruction-following/).

## Further Reading

Paper:
- [*Learning to summarize from human feedback*](https://arxiv.org/abs/2009.01325), [OpenAI blog post](https://openai.com/research/learning-to-summarize-with-human-feedback), [Paper Explained](https://www.youtube.com/watch?v=vLTmnaMpQCs)

Posts:
- [Illustrating Reinforcement Learning from Human Feedback (RLHF)](https://huggingface.co/blog/rlhf)
- [RLHF: Reinforcement Learning from Human Feedback](https://huyenchip.com/2023/05/02/rlhf.html)
- [Understanding Reinforcement Learning from Human Feedback (RLHF)](https://wandb.ai/ayush-thakur/RLHF/reports/Understanding-Reinforcement-Learning-from-Human-Feedback-RLHF-Part-1--VmlldzoyODk5MTIx)

PPO:
- [Hugging Face RL Course: PPO](https://huggingface.co/learn/deep-rl-course/unit8/introduction)
- [Overview of the two main PPO contributions](https://stackoverflow.com/a/50663200)
- [The 37 Implementation Details of Proximal Policy Optimization](https://ppo-details.cleanrl.dev//2021/11/05/ppo-implementation-details/) (includes ~1 hour of code implementation videos)
- [TRPO and PPO lecture by Pieter Abbeel](https://youtu.be/KjWF8VIMGiY)
- [CleanRL's PPO implementation](https://docs.cleanrl.dev/rl-algorithms/ppo/)