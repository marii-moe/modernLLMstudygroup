# LoRA: Low-Rank Adaptation of Large Language Models

This week's paper is [*LoRA: Low-Rank Adaptation of Large Language Models*](https://arxiv.org/abs/2106.09685).

Further Reading:
- [How to fine-tune a Transformer (pt. 2, LoRA)](https://radekosmulski.com/how-to-fine-tune-a-tranformer-pt-2)
- [*QLoRA: Efficient Finetuning of Quantized LLMs*](https://arxiv.org/abs/2305.14314)

Libraries:
- [PEFT](https://huggingface.co/docs/peft/index)
- [Authors Repo](https://github.com/microsoft/LoRA)