# The Pile: An 800GB Dataset of Diverse Text for Language Modeling

This week's paper is [*The Pile: An 800GB Dataset of Diverse Text for Language Modeling*](https://arxiv.org/abs/2101.00027) a 825 GiB English text corpus targeted at training large-scale language models. Outside of C4, The Pile was one of the first open released datasets for training LLMs.

Further Reading:
* C4: [*Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer*](https://arxiv.org/abs/1910.10683), [AllenAI's C4](https://huggingface.co/datasets/allenai/c4)
* [The RefinedWeb Dataset for Falcon LLM: Outperforming Curated Corpora with Web Data, and Web Data Only](https://arxiv.org/abs/2306.01116), [dataset](https://huggingface.co/datasets/tiiuae/falcon-refinedweb)
* [RedPajama](https://huggingface.co/datasets/togethercomputer/RedPajama-Data-1T)
* [SlimPajama: A 627B token, cleaned and deduplicated version of RedPajama](https://www.cerebras.net/blog/slimpajama-a-627b-token-cleaned-and-deduplicated-version-of-redpajama), [dataset](https://huggingface.co/datasets/cerebras/SlimPajama-627B)
* [The Stack: 6 TB of permissive code data](https://huggingface.co/datasets/bigcode/the-stack)
* [RedPajama-Data-v2: An open dataset with 30 trillion tokens for training large language models](https://www.together.ai/blog/redpajama-data-v2), [dataset](https://huggingface.co/datasets/togethercomputer/RedPajama-Data-V2)